{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "import json\n",
    "import pandas as pd\n",
    "jsonl = []\n",
    "with open('/home/ubuntu/efs/lei/emerald_new/test.jsonl') as f:\n",
    "    for i,line in enumerate(f):\n",
    "#         print(i)\n",
    "        jsonl.append(json.loads(line.strip()))\n",
    "df = pd.DataFrame({'jsonl': jsonl})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fulltext_length(obj):\n",
    "    text = ' '.join(i for i in obj['section_names'] if i != '__NO_TITLE__') + ' ' + ' '.join(par for sec in obj['sections'] for par in sec)\n",
    "    return len(text.split())\n",
    "\n",
    "def fulltext(obj):\n",
    "    text = ' '.join(par for sec in obj['sections'] for par in sec)\n",
    "    return text\n",
    "\n",
    "def fulltext_sec(obj):\n",
    "    text = [par for sec in obj['sections'] for par in sec]\n",
    "    return text\n",
    "\n",
    "def abstract_sec(obj):\n",
    "    text = [par for sec in obj['abstract_sections'] for par in sec]\n",
    "    return text\n",
    "\n",
    "def fullabs(obj):\n",
    "    text = ' '.join(par for sec in obj['abstract_sections'] for par in sec)\n",
    "    return text\n",
    "\n",
    "df['fulltext_length'] = df.jsonl.apply(fulltext_length)\n",
    "df['fulltext']= df.jsonl.apply(fulltext)\n",
    "df['fullabs']= df.jsonl.apply(fullabs)\n",
    "df['fulltext_sec']= df.jsonl.apply(fulltext_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = jsonl[0]\n",
    "# line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1d3bc12892a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfulltext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;31m# text\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "text = df.fulltext[0]\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'- The purpose of this paper is to report on a review of the writing practices and experiences of scholars who have published qualitative papers in the field of entrepreneurship. It evaluates existing knowledge about how \"well-published\" entrepreneurship scholars go about writing up qualitative research. It identifies the antecedents, processes, and consequences of qualitative research authorship as self-described by authors. - Scholars who had published qualitative papers in the five top-ranked entrepreneurship journals over a 20-year period were asked to complete a qualitative survey about their writing practices. A qualitative analysis of 37 usable replies was undertaken. - Entrepreneurship scholars perceive their qualitative research writing to be more enriching and philosophical than quantitative research. Although they feel strong connections with their research subjects, they find qualitative research difficult and time consuming to write up. It is hard to bridge the gap between working with large amounts of transcribed data and the editorial requirements of journals, without losing the vitality of data. Qualitative research and subsequent writing skills have often been learned by trial and error. Many are inspired by specific texts, which may include novels, poems or plays. - This work shows how useful it is to discuss qualitative writing processes so that we may learn from the \"blood, toil, tears and sweat\" of those who have already successfully navigated both the writing and publishing of qualitative research. - Although there is a vigorous debate within the entrepreneurship literature about the prevalence and suitability of different methods and methodological approaches, there is no explicit discussion of how researchers engage with writing up qualitative research for publication. The paper addresses this gap and shares insights and guidance from our community of practice.'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abst  = df.fullabs[0]\n",
    "# abst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yued/miniconda3/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (306) is lower than number of sentences (316). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "words=196\n",
    "def summ_lsa(text):\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    c = 0\n",
    "    s = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        if c < words:\n",
    "            s.append(str(sentence))\n",
    "            c+= len(str(sentence).split(' '))\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(s)\n",
    "# summ_lsa(text)\n",
    "df['lsa_summ']= df.fulltext.apply(summ_lsa)\n",
    "\n",
    "hyp = df.lsa_summ.tolist()\n",
    "hyp= [i.replace('\\n',' ') for i in hyp]\n",
    "\n",
    "hyp_path = '../result/lsa.txt'\n",
    "write_output(hyp,hyp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "def summ_textrank(text):\n",
    "    return summarizer.summarize(text, words=196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['textrank_summ']= df.fulltext.apply(summ_textrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.textrank_summ[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(output_list,output_path):\n",
    "    with open(output_path,'w') as f:\n",
    "        for i,l in enumerate(output_list):\n",
    "            f.write(l+'\\n')\n",
    "    print('Write %d lines into %s'%(i,output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = df.textrank_summ.tolist()\n",
    "hyp= [i.replace('\\n',' ') for i in hyp]\n",
    "ref = df.fullabs.tolist()\n",
    "ref_path = '../result/ref.txt'\n",
    "write_output(ref,ref_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer as Summarizer\n",
    "\n",
    "words=196\n",
    "def summ_traditional(text):\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    c = 0\n",
    "    s = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        if c < words:\n",
    "            s.append(str(sentence))\n",
    "            c+= len(str(sentence).split(' '))\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(s)\n",
    "# summ_lsa(text)\n",
    "df['lexrank_summ']= df.fulltext.apply(summ_traditional)\n",
    "\n",
    "hyp = df.lexrank_summ.tolist()\n",
    "hyp= [i.replace('\\n',' ') for i in hyp]\n",
    "\n",
    "hyp_path = '../result/lexrank.txt'\n",
    "write_output(hyp,hyp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize SumBasics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.sum_basic import SumBasicSummarizer as Summarizer\n",
    "# summ_lsa(text)\n",
    "def summ_traditional(text):\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    c = 0\n",
    "    s = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        if c < words:\n",
    "            s.append(str(sentence))\n",
    "            c+= len(str(sentence).split(' '))\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(s)\n",
    "df['sumbasic_summ']= df.fulltext.apply(summ_traditional)\n",
    "\n",
    "hyp = df.sumbasic_summ.tolist()\n",
    "hyp= [i.replace('\\n',' ') for i in hyp]\n",
    "\n",
    "hyp_path = '../result/sumbasic.txt'\n",
    "write_output(hyp,hyp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_lines(hyp_path):\n",
    "    # replace empty lines\n",
    "    with open(hyp_path,'r') as f:\n",
    "        hyps = f.readlines()\n",
    "\n",
    "    new_hyps=[]\n",
    "    for idx,l in enumerate(hyps):\n",
    "        if l == \"\\n\" or len(l.strip())==0:\n",
    "            new_hyps.append('none')\n",
    "        else:\n",
    "            new_hyps.append(l.strip())\n",
    "\n",
    "    write_output(new_hyps,hyp_path+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
