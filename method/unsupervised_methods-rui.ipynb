{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "import json\n",
    "import pandas as pd\n",
    "    \n",
    "def lines2df(lines):\n",
    "    ex_dicts = []\n",
    "    for line in lines:\n",
    "        ex_dict = json.loads(line.strip())\n",
    "        ex_dicts.append(ex_dict)\n",
    "    df = pd.DataFrame.from_records(ex_dicts, columns=list(ex_dicts[0].keys()))\n",
    "    return df\n",
    "\n",
    "lines = [l for l in open('/home/ubuntu/efs/lei/emerald_new/test.jsonl', 'r').readlines()]\n",
    "\n",
    "emerald_df = lines2df(lines)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_paragraph(sections):\n",
    "    return '\\n'.join(j for i in sections for j in i)\n",
    "emerald_df['fulltext'] = emerald_df.sections.apply(concat_paragraph)\n",
    "emerald_df['abstracttext'] = emerald_df.abstract_sections.apply(concat_paragraph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_section_title_like(section_names, section_text, cuewords, include_title=True):\n",
    "    text = []\n",
    "    for sn, st in zip(section_names, section_text):\n",
    "        sn = sn.lower()\n",
    "        for cueword in cuewords:\n",
    "            if cueword in sn:\n",
    "                if include_title:\n",
    "                    text.append(sn)\n",
    "                text.append('\\n'.join(st))\n",
    "                break\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "def find_section_title_not_like(section_names, section_text, cuewords, include_title=True):\n",
    "    text = []\n",
    "    for sn, st in zip(section_names, section_text):\n",
    "        sn = sn.lower()\n",
    "        positive = False\n",
    "        for cueword in cuewords:\n",
    "            if cueword in sn:\n",
    "                positive = True\n",
    "                break\n",
    "        if not positive:\n",
    "            if include_title:\n",
    "                text.append(sn)\n",
    "            text.append('\\n'.join(st))\n",
    "    return '\\n'.join(text)\n",
    "\n",
    "\n",
    "emerald_df['src-full'] = emerald_df['fulltext']\n",
    "emerald_df['src-intro'] = emerald_df.apply(lambda row: find_section_title_like(row['section_names'], row['sections'], ['intro', 'purpose']), axis=1)\n",
    "emerald_df['src-design'] = emerald_df.apply(lambda row: find_section_title_like(row['section_names'], row['sections'], ['design', 'method', 'approach']), axis=1)\n",
    "emerald_df['src-result'] = emerald_df.apply(lambda row: find_section_title_like(row['section_names'], row['sections'], ['result', 'find', 'discuss', 'analy']), axis=1)\n",
    "emerald_df['src-conclu'] = emerald_df.apply(lambda row: find_section_title_like(row['section_names'], row['sections'], ['conclu', 'future']), axis=1)\n",
    "emerald_df['src-related'] = emerald_df.apply(lambda row: find_section_title_like(row['section_names'], row['sections'], ['related work', 'literat', 'background']), axis=1)\n",
    "\n",
    "emerald_df['src-IC'] = emerald_df['src-intro'] + '\\n' + emerald_df['src-conclu']\n",
    "\n",
    "# ('Purpose', 'Design/methodology/approach', 'Findings', 'Originality/value')\n",
    "emerald_df['tgt-full'] = emerald_df['abstracttext']\n",
    "emerald_df['tgt-intro'] = emerald_df.apply(lambda row: find_section_title_like(row['abstract_sections_names'], row['abstract_sections'],\n",
    "                                                                                         ['Purpose'.lower()], False), axis=1)\n",
    "emerald_df['tgt-design'] = emerald_df.apply(lambda row: find_section_title_like(row['abstract_sections_names'], row['abstract_sections'],\n",
    "                                                                                   ['Design/methodology/approach'.lower()], False), axis=1)\n",
    "emerald_df['tgt-find'] = emerald_df.apply(lambda row: find_section_title_like(row['abstract_sections_names'], row['abstract_sections'],\n",
    "                                                                                     ['Findings'.lower()], False), axis=1)\n",
    "emerald_df['tgt-origin'] = emerald_df.apply(lambda row: find_section_title_like(row['abstract_sections_names'], row['abstract_sections'],\n",
    "                                                                                        ['Originality/value'.lower()], False),axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import spacy\n",
    "from pysbd.utils import PySBDFactory\n",
    "\n",
    "# spacy_nlp = spacy.load('en_core_web_sm')\n",
    "spacy_nlp = spacy.blank('en')\n",
    "spacy_nlp.add_pipe(PySBDFactory(spacy_nlp))\n",
    "from functools import partial\n",
    "\n",
    "src2tgt_name_map = [\n",
    "    ('src-full', 'tgt-full'),\n",
    "    ('src-intro', 'tgt-intro'),\n",
    "    ('src-design', 'tgt-design'),\n",
    "    ('src-result', 'tgt-find'),\n",
    "    ('src-IC', 'tgt-origin'),\n",
    "]\n",
    "\n",
    "tgt_sent_map = {'tgt-full': 10, 'tgt-intro': 2, 'tgt-design': 2, 'tgt-find': 3, 'tgt-origin': 2}\n",
    "tgt_word_map = {'tgt-full': 271, 'tgt-intro': 54, 'tgt-design': 52, 'tgt-find': 68, 'tgt-origin': 47}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine #word/#sent with dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tgt-full, #dp=6000, #avgsent=10.22, #avgword=271.06\n",
      "**************************************************\n",
      "tgt-intro, #dp=6000, #avgsent=1.83, #avgword=53.56\n",
      "**************************************************\n",
      "tgt-design, #dp=6000, #avgsent=2.13, #avgword=52.16\n",
      "**************************************************\n",
      "tgt-find, #dp=6000, #avgsent=2.60, #avgword=68.48\n",
      "**************************************************\n",
      "tgt-origin, #dp=6000, #avgsent=1.79, #avgword=47.01\n",
      "**************************************************\n"
     ]
    }
   ],
   "source": [
    "lines = [l for l in open('/home/ubuntu/efs/emerald/dev.jsonl', 'r').readlines()]\n",
    "dev_df = lines2df(lines)\n",
    "\n",
    "dev_df['fulltext'] = dev_df.sections.apply(concat_paragraph)\n",
    "dev_df['abstracttext'] = dev_df.abstract_sections.apply(concat_paragraph)\n",
    "\n",
    "dev_df['tgt-full'] = dev_df['abstracttext']\n",
    "dev_df['tgt-intro'] = dev_df.apply(lambda row: find_section_title_like(row['abstract_sections_names'], row['abstract_sections'], ['Purpose'.lower()], False), axis=1)\n",
    "dev_df['tgt-design'] = dev_df.apply(lambda row: find_section_title_like(row['abstract_sections_names'], row['abstract_sections'], ['Design/methodology/approach'.lower()], False), axis=1)\n",
    "dev_df['tgt-find'] = dev_df.apply(lambda row: find_section_title_like(row['abstract_sections_names'], row['abstract_sections'], ['Findings'.lower()], False), axis=1)\n",
    "dev_df['tgt-origin'] = dev_df.apply(lambda row: find_section_title_like(row['abstract_sections_names'], row['abstract_sections'], ['Originality/value'.lower()], False),axis=1)\n",
    "\n",
    "tgt_col_names = ['tgt-full', 'tgt-intro', 'tgt-design', 'tgt-find', 'tgt-origin']\n",
    "\n",
    "for tgt_col_name in tgt_col_names:\n",
    "    tgts = [[[w.text for w in sent] for sent in spacy_nlp(r[tgt_col_name]).sents] for _, r in list(emerald_df.iterrows())]\n",
    "    num_sent = [len(sents) for sents in tgts]\n",
    "    len_word = [np.sum([len(sent) for sent in sents]) for sents in tgts]\n",
    "    print('%s, #dp=%d, #avgsent=%.2f, #avgword=%.2f' % (tgt_col_name, len(tgts), np.mean(num_sent), np.mean(len_word)))\n",
    "#     for sents in tgts:\n",
    "#         for sent in sents:\n",
    "#             print(len(sent), ' '.join(sent))\n",
    "#         print('*' * 20)\n",
    "    print('*' * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'keywords', 'url', 'section_names', 'sections',\n",
       "       'abstract_sections_names', 'abstract_sections', 'references',\n",
       "       'appendix', 'journal', 'id', 'category', 'fulltext', 'abstracttext',\n",
       "       'src-full', 'src-intro', 'src-design', 'src-result', 'src-conclu',\n",
       "       'src-related', 'src-IC', 'tgt-full', 'tgt-intro', 'tgt-design',\n",
       "       'tgt-find', 'tgt-origin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emerald_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = jsonl[0]\n",
    "# line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = emerald_df.fulltext[0]\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "abst  = emerald_df.fullabs[0]\n",
    "# abst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Summarizing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (437) is lower than number of sentences (511). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (398) is lower than number of sentences (1120). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (306) is lower than number of sentences (387). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (624) is lower than number of sentences (848). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (468) is lower than number of sentences (585). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (479) is lower than number of sentences (749). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (349) is lower than number of sentences (492). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (27) is lower than number of sentences (80). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (111) is lower than number of sentences (155). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (213) is lower than number of sentences (980). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (136) is lower than number of sentences (175). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n",
      "/home/ubuntu/efs/.conda/kp/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (143) is lower than number of sentences (150). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "import sumy\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "\n",
    "def summ_lsa(text, tgt_word):\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    c = 0\n",
    "    s = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        if c < tgt_word:\n",
    "            s.append(str(sentence))\n",
    "            c+= len(str(sentence).split(' '))\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(s)\n",
    "\n",
    "# summ_lsa(text)\n",
    "for src_sec, tgt_sec in src2tgt_name_map:\n",
    "    tgt_word = tgt_word_map[tgt_sec]\n",
    "    summ_lsa_fn = partial(summ_lsa, tgt_word=tgt_word)\n",
    "    emerald_df['lsa_summ.' + src_sec]= emerald_df[src_sec].apply(summ_lsa_fn)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "def summ_textrank(text, tgt_word):\n",
    "    return summarizer.summarize(text, words=tgt_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for src_sec, tgt_sec in src2tgt_name_map:\n",
    "    summ_textrank_fn = partial(summ_textrank, tgt_word=tgt_word_map[tgt_sec])\n",
    "    emerald_df['textrank_summ.' + src_sec]= emerald_df[src_sec].apply(summ_textrank_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On the other hand, FMS focuses on implementing certain dimensions, such as labor, machine, operations and material handling flexibilities, which allows the manufacturing system to absorb problems with little or no impact to overall system performance or throughput (Gunasekaran et al., 2008; Harrison, 1997; Hazen et al., 2017; Hormozi, 2001).\\nDriven by the lack of consensus regarding the relationship between lean and flexibility, and their impact on operational performance metrics, this paper examines the mediating effect of LM on the relationship between FMS dimensions and operational performance metrics.\\nSeveral empirical studies have considered the impact of flexibility dimensions on certain operational performance metrics (Inman et al., 2011; Oke, 2013; Purvis et al., 2014; Wei et al., 2017).\\nwe consulted with facility managers who were involved in LM and FMS implementation to determine which lean and flexibility dimensions have been implemented and what operational performance metrics have been utilized.\\nThe breadth and depth of literature found on the topic relating lean, FMS and operational performance metrics is limited to few dimensions, such as the work presented by Christopher and Towill (2001), El-khalil (2009), Goldman and Nagel (1993), Goldsby et al.\\nThis study clearly indicates that LM plays a mediator role in the relationship between flexibility implementation and operational performance metrics, as illustrated in Table VI.\\nAlthough previous literature indicates a positive impact of lean and flexibility on performance metrics, some studies suggest that this relationship is situational (Goldsby et al., 2006).\\nThe empirical evidence presented in this study clearly shows the mediating role that LM plays in the relationship between FMS implementation and operational performance metrics in the automotive manufacturing industry.'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emerald_df['textrank_summ.src-full'][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer as Summarizer\n",
    "\n",
    "def summ_lexrank(text, tgt_word):\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    c = 0\n",
    "    s = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        if c < tgt_word:\n",
    "            s.append(str(sentence))\n",
    "            c+= len(str(sentence).split(' '))\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(s)\n",
    "# summ_lsa(text)\n",
    "# emerald_df['lexrank_summ']= emerald_df.fulltext.apply(summ_traditional)\n",
    "\n",
    "for src_sec, tgt_sec in src2tgt_name_map:\n",
    "    summ_lexrank_fn = partial(summ_lexrank, tgt_word=tgt_word_map[tgt_sec])\n",
    "    emerald_df['lexrank_summ.' + src_sec]= emerald_df[src_sec].apply(summ_lexrank_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize SumBasics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.summarizers.sum_basic import SumBasicSummarizer as Summarizer\n",
    "# summ_lsa(text)\n",
    "def summ_sumbasic(text, tgt_word):\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    c = 0\n",
    "    s = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        if c < tgt_word:\n",
    "            s.append(str(sentence))\n",
    "            c+= len(str(sentence).split(' '))\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(s)\n",
    "# emerald_df['sumbasic_summ']= emerald_df.fulltext.apply(summ_sumbasic)\n",
    "\n",
    "for src_sec, tgt_sec in src2tgt_name_map:\n",
    "    summ_sumbasic_fn = partial(summ_sumbasic, tgt_word=tgt_word_map[tgt_sec])\n",
    "    emerald_df['sumbasic_summ.' + src_sec]= emerald_df[src_sec].apply(summ_sumbasic_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'keywords', 'url', 'section_names', 'sections',\n",
       "       'abstract_sections_names', 'abstract_sections', 'references',\n",
       "       'appendix', 'journal', 'id', 'category', 'fulltext', 'abstracttext',\n",
       "       'src-full', 'src-intro', 'src-design', 'src-result', 'src-conclu',\n",
       "       'src-related', 'src-IC', 'tgt-full', 'tgt-intro', 'tgt-design',\n",
       "       'tgt-find', 'tgt-origin', 'leadK.src-full', 'leadK.src-intro',\n",
       "       'leadK.src-design', 'leadK.src-result', 'leadK.src-IC',\n",
       "       'tailK.src-full', 'tailK.src-intro', 'tailK.src-design',\n",
       "       'tailK.src-result', 'tailK.src-IC', 'lsa_summ.src-full',\n",
       "       'lsa_summ.src-intro', 'lsa_summ.src-design', 'lsa_summ.src-result',\n",
       "       'lsa_summ.src-IC', 'textrank_summ.src-full', 'textrank_summ.src-intro',\n",
       "       'textrank_summ.src-design', 'textrank_summ.src-result',\n",
       "       'textrank_summ.src-IC', 'lexrank_summ.src-full',\n",
       "       'lexrank_summ.src-intro', 'lexrank_summ.src-design',\n",
       "       'lexrank_summ.src-result', 'lexrank_summ.src-IC',\n",
       "       'sumbasic_summ.src-full', 'sumbasic_summ.src-intro',\n",
       "       'sumbasic_summ.src-design', 'sumbasic_summ.src-result',\n",
       "       'sumbasic_summ.src-IC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emerald_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LEAD-k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src-full tgt-full\n",
      "src-intro tgt-intro\n",
      "src-design tgt-design\n",
      "src-result tgt-find\n",
      "src-IC tgt-origin\n"
     ]
    }
   ],
   "source": [
    "def summ_lead_tail_K(text, num_tgt_sent, is_tail=False):\n",
    "    tgt_sents = []\n",
    "    sents = spacy_nlp(text).sents\n",
    "    if is_tail:\n",
    "        sents = list(sents)[::-1]\n",
    "    for i, sent in enumerate(sents):\n",
    "        if len(tgt_sents) > num_tgt_sent:\n",
    "            break\n",
    "        sent = ' '.join([w.text for w in sent])\n",
    "        tgt_sents.append(sent)\n",
    "        \n",
    "    return ' '.join(tgt_sents)\n",
    "\n",
    "# for src_sec, tgt_sec in src2tgt_name_map[1:]:\n",
    "#     print(src_sec, tgt_sec)\n",
    "#     summ_lead_K_fn = partial(summ_lead_tail_K, num_tgt_sent=tgt_sent_map[tgt_sec], is_tail=False)\n",
    "#     emerald_df['leadK.' + src_sec] = emerald_df[src_sec].apply(summ_lead_K_fn)\n",
    "\n",
    "for src_sec, tgt_sec in src2tgt_name_map:\n",
    "    print(src_sec, tgt_sec)\n",
    "    summ_tail_K_fn = partial(summ_lead_tail_K, num_tgt_sent=tgt_sent_map[tgt_sec], is_tail=True)\n",
    "    emerald_df['tailK.' + src_sec] = emerald_df[src_sec].apply(summ_tail_K_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['title', 'keywords', 'url', 'section_names', 'sections',\n",
       "       'abstract_sections_names', 'abstract_sections', 'references',\n",
       "       'appendix', 'journal', 'id', 'category', 'fulltext', 'abstracttext',\n",
       "       'src-full', 'src-intro', 'src-design', 'src-result', 'src-conclu',\n",
       "       'src-related', 'src-IC', 'tgt-full', 'tgt-intro', 'tgt-design',\n",
       "       'tgt-find', 'tgt-origin', 'leadK.src-full', 'leadK.src-intro',\n",
       "       'leadK.src-design', 'leadK.src-result', 'leadK.src-IC',\n",
       "       'tailK.src-full', 'tailK.src-intro', 'tailK.src-design',\n",
       "       'tailK.src-result', 'tailK.src-IC'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emerald_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dump output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lsa_summ.src-full.tgt-full.txt #word=291\n",
      "lsa_summ.src-intro.tgt-intro.txt #word=61\n",
      "lsa_summ.src-design.tgt-design.txt #word=45\n",
      "lsa_summ.src-result.tgt-find.txt #word=63\n",
      "lsa_summ.src-IC.tgt-origin.txt #word=60\n",
      "textrank_summ.src-full.tgt-full.txt #word=271\n",
      "textrank_summ.src-intro.tgt-intro.txt #word=46\n",
      "textrank_summ.src-design.tgt-design.txt #word=34\n",
      "textrank_summ.src-result.tgt-find.txt #word=49\n",
      "textrank_summ.src-IC.tgt-origin.txt #word=43\n",
      "lexrank_summ.src-full.tgt-full.txt #word=293\n",
      "lexrank_summ.src-intro.tgt-intro.txt #word=60\n",
      "lexrank_summ.src-design.tgt-design.txt #word=46\n",
      "lexrank_summ.src-result.tgt-find.txt #word=63\n",
      "lexrank_summ.src-IC.tgt-origin.txt #word=60\n",
      "sumbasic_summ.src-full.tgt-full.txt #word=281\n",
      "sumbasic_summ.src-intro.tgt-intro.txt #word=60\n",
      "sumbasic_summ.src-design.tgt-design.txt #word=44\n",
      "sumbasic_summ.src-result.tgt-find.txt #word=60\n",
      "sumbasic_summ.src-IC.tgt-origin.txt #word=59\n",
      "leadK.src-full.tgt-full.txt #word=391\n",
      "leadK.src-intro.tgt-intro.txt #word=59\n",
      "leadK.src-design.tgt-design.txt #word=34\n",
      "leadK.src-result.tgt-find.txt #word=58\n",
      "leadK.src-IC.tgt-origin.txt #word=62\n",
      "tailK.src-full.tgt-full.txt #word=239\n",
      "tailK.src-intro.tgt-intro.txt #word=73\n",
      "tailK.src-design.tgt-design.txt #word=58\n",
      "tailK.src-result.tgt-find.txt #word=86\n",
      "tailK.src-IC.tgt-origin.txt #word=59\n"
     ]
    }
   ],
   "source": [
    "### check summary length\n",
    "model_names = ['lsa_summ', 'textrank_summ', 'lexrank_summ', 'sumbasic_summ', 'leadK', 'tailK']\n",
    "# model_names = ['leadK', 'tailK']\n",
    "for model_name in model_names:\n",
    "    for src_sec, tgt_sec in src2tgt_name_map:\n",
    "        hyps = emerald_df[model_name + '.' + src_sec].tolist()\n",
    "        num_words = [len(hyp.split()) for hyp in hyps]\n",
    "        \n",
    "        print('%s.%s.%s.txt' % (model_name, src_sec, tgt_sec), '#word=%d' % np.mean(num_words))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-full.tgt-full.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-full.tgt-full.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-intro.tgt-intro.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-intro.tgt-intro.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-design.tgt-design.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-design.tgt-design.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-result.tgt-find.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-result.tgt-find.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-IC.tgt-origin.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lsa_summ.src-IC.tgt-origin.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-full.tgt-full.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-full.tgt-full.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-intro.tgt-intro.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-intro.tgt-intro.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-design.tgt-design.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-design.tgt-design.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-result.tgt-find.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-result.tgt-find.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-IC.tgt-origin.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/textrank_summ.src-IC.tgt-origin.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-full.tgt-full.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-full.tgt-full.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-intro.tgt-intro.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-intro.tgt-intro.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-design.tgt-design.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-design.tgt-design.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-result.tgt-find.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-result.tgt-find.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-IC.tgt-origin.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/lexrank_summ.src-IC.tgt-origin.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-full.tgt-full.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-full.tgt-full.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-intro.tgt-intro.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-intro.tgt-intro.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-design.tgt-design.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-design.tgt-design.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-result.tgt-find.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-result.tgt-find.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-IC.tgt-origin.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/sumbasic_summ.src-IC.tgt-origin.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-full.tgt-full.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-full.tgt-full.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-intro.tgt-intro.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-intro.tgt-intro.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-design.tgt-design.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-design.tgt-design.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-result.tgt-find.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-result.tgt-find.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-IC.tgt-origin.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/leadK.src-IC.tgt-origin.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-full.tgt-full.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-full.tgt-full.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-intro.tgt-intro.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-intro.tgt-intro.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-design.tgt-design.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-design.tgt-design.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-result.tgt-find.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-result.tgt-find.txt, #result=6000\n",
      "Write 6000 lines into /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-IC.tgt-origin.txt\n",
      "Dumped to /home/ubuntu/efs/rum20/long_summ/result/rui/tailK.src-IC.tgt-origin.txt, #result=6000\n"
     ]
    }
   ],
   "source": [
    "def write_output(output_list, output_path):\n",
    "    with open(output_path,'w') as f:\n",
    "        for i, l in enumerate(output_list):\n",
    "            f.write(l+'\\n')\n",
    "    print('Write %d lines into %s' % (i + 1, output_path))\n",
    "\n",
    "output_dir = '/home/ubuntu/efs/rum20/long_summ/result/rui/'\n",
    "\n",
    "model_names = ['lsa_summ', 'textrank_summ', 'lexrank_summ', 'sumbasic_summ', 'leadK', 'tailK']\n",
    "for model_name in model_names:\n",
    "    for src_sec, tgt_sec in src2tgt_name_map:\n",
    "        hyps = emerald_df[model_name + '.' + src_sec].tolist()\n",
    "        \n",
    "        new_hyps = []\n",
    "        for i in range(len(hyps)):\n",
    "            l = hyps[i].strip()\n",
    "            if l == \"\\n\" or len(l.strip())==0:\n",
    "                l = '__NONE__'\n",
    "            l = l.replace('\\n',' ')\n",
    "            new_hyps.append(l)\n",
    "\n",
    "        output_path = output_dir + '%s.%s.%s.txt' % (model_name, src_sec, tgt_sec)\n",
    "        write_output(new_hyps, output_path)\n",
    "\n",
    "        print('Dumped to %s, #result=%d' % (output_path, len(new_hyps)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
