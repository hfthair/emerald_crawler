{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read data \n",
    "import json\n",
    "import pandas as pd\n",
    "jsonl = []\n",
    "with open('../data/test.jsonl') as f:\n",
    "    for i,line in enumerate(f):\n",
    "#         print(i)\n",
    "        jsonl.append(json.loads(line.strip()))\n",
    "df = pd.DataFrame({'jsonl': jsonl})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fulltext_length(obj):\n",
    "    text = ' '.join(i for i in obj['section_names'] if i != '__NO_TITLE__') + ' ' + ' '.join(par for sec in obj['sections'] for par in sec)\n",
    "    return len(text.split())\n",
    "\n",
    "def fulltext(obj):\n",
    "    text = ' '.join(par for sec in obj['sections'] for par in sec)\n",
    "    return text\n",
    "\n",
    "def fulltext_sec(obj):\n",
    "    text = [par for sec in obj['sections'] for par in sec]\n",
    "    return text\n",
    "\n",
    "def abstract_sec(obj):\n",
    "    text = [par for sec in obj['abstract_sections'] for par in sec]\n",
    "    return text\n",
    "\n",
    "def fullabs(obj):\n",
    "    text = ' '.join(par for sec in obj['abstract_sections'] for par in sec)\n",
    "    return text\n",
    "\n",
    "df['fulltext_length'] = df.jsonl.apply(fulltext_length)\n",
    "df['fulltext']= df.jsonl.apply(fulltext)\n",
    "df['fullabs']= df.jsonl.apply(fullabs)\n",
    "df['fulltext_sec']= df.jsonl.apply(fulltext_sec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "line = jsonl[0]\n",
    "# line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = df.fulltext[0]\n",
    "# text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "abst  = df.fullabs[0]\n",
    "#abst"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/yued/miniconda3/lib/python3.7/site-packages/sumy/summarizers/lsa.py:76: UserWarning: Number of words (306) is lower than number of sentences (316). LSA algorithm may not work properly.\n",
      "  warn(message % (words_count, sentences_count))\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division, print_function, unicode_literals\n",
    "\n",
    "from sumy.parsers.html import HtmlParser\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer as Summarizer\n",
    "from sumy.nlp.stemmers import Stemmer\n",
    "from sumy.utils import get_stop_words\n",
    "\n",
    "words=196\n",
    "def summ_lsa(text):\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    c = 0\n",
    "    s = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        if c < words:\n",
    "            s.append(str(sentence))\n",
    "            c+= len(str(sentence).split(' '))\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(s)\n",
    "# summ_lsa(text)\n",
    "df['lsa_summ']= df.fulltext.apply(summ_lsa)\n",
    "\n",
    "hyp = df.lsa_summ.tolist()\n",
    "hyp= [i.replace('\\n',' ') for i in hyp]\n",
    "\n",
    "hyp_path = '../result/lsa.txt'\n",
    "write_output(hyp,hyp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize TextRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from summa import summarizer\n",
    "def summ_textrank(text):\n",
    "    return summarizer.summarize(text, words=196)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['textrank_summ']= df.fulltext.apply(summ_textrank)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.textrank_summ[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Writting Output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_output(output_list,output_path):\n",
    "    with open(output_path,'w') as f:\n",
    "        for i,l in enumerate(output_list):\n",
    "            f.write(l+'\\n')\n",
    "    print('Write %d lines into %s'%(i,output_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp = df.textrank_summ.tolist()\n",
    "hyp= [i.replace('\\n',' ') for i in hyp]\n",
    "ref = df.fullabs.tolist()\n",
    "ref_path = '../result/ref.txt'\n",
    "write_output(ref,ref_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize LexRank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 5999 lines into ../result/lexrank.txt\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.lex_rank import LexRankSummarizer as Summarizer\n",
    "\n",
    "words=196\n",
    "def summ_traditional(text):\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    c = 0\n",
    "    s = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        if c < words:\n",
    "            s.append(str(sentence))\n",
    "            c+= len(str(sentence).split(' '))\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(s)\n",
    "# summ_lsa(text)\n",
    "df['lexrank_summ']= df.fulltext.apply(summ_traditional)\n",
    "\n",
    "hyp = df.lexrank_summ.tolist()\n",
    "hyp= [i.replace('\\n',' ') for i in hyp]\n",
    "\n",
    "hyp_path = '../result/lexrank.txt'\n",
    "write_output(hyp,hyp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarize SumBasics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Write 5999 lines into ../result/sumbasic.txt\n"
     ]
    }
   ],
   "source": [
    "from sumy.summarizers.sum_basic import SumBasicSummarizer as Summarizer\n",
    "# summ_lsa(text)\n",
    "def summ_traditional(text):\n",
    "    LANGUAGE = \"english\"\n",
    "    SENTENCES_COUNT = 20\n",
    "    parser = PlaintextParser.from_string(text, Tokenizer(LANGUAGE))\n",
    "    stemmer = Stemmer(LANGUAGE)\n",
    "\n",
    "    summarizer = Summarizer(stemmer)\n",
    "    summarizer.stop_words = get_stop_words(LANGUAGE)\n",
    "    \n",
    "    c = 0\n",
    "    s = []\n",
    "    for sentence in summarizer(parser.document, SENTENCES_COUNT):\n",
    "        if c < words:\n",
    "            s.append(str(sentence))\n",
    "            c+= len(str(sentence).split(' '))\n",
    "        else:\n",
    "            break\n",
    "    return \" \".join(s)\n",
    "df['sumbasic_summ']= df.fulltext.apply(summ_traditional)\n",
    "\n",
    "hyp = df.sumbasic_summ.tolist()\n",
    "hyp= [i.replace('\\n',' ') for i in hyp]\n",
    "\n",
    "hyp_path = '../result/sumbasic.txt'\n",
    "write_output(hyp,hyp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove empty lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_empty_lines(hyp_path):\n",
    "    # replace empty lines\n",
    "    with open(hyp_path,'r') as f:\n",
    "        hyps = f.readlines()\n",
    "\n",
    "    new_hyps=[]\n",
    "    for idx,l in enumerate(hyps):\n",
    "        if l == \"\\n\" or len(l.strip())==0:\n",
    "            new_hyps.append('none')\n",
    "        else:\n",
    "            new_hyps.append(l.strip())\n",
    "\n",
    "    write_output(new_hyps,hyp_path+'.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
